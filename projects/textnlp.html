<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131669567-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131669567-1');
</script>
<!-- End google Code -->
<!-- Matomo -->
<script type="text/javascript">
  var _paq = _paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
  _paq.push(["setCookieDomain", "*.jlivingston01.github.io"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://jlivingston01.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src='//cdn.matomo.cloud/jlivingston01.matomo.cloud/piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->
<!-- Matomo Image Tracker-->
<noscript>
<img src="https://jlivingston01.matomo.cloud/piwik.php?idsite=1&amp;rec=1&amp;action_name=machinelearning" style="border:0" alt="" />
</noscript>
<!-- End Matomo -->
</head>
<body>
<header id="header" class="alt">
					<h1><a href="../index.html">JLIV's Github Site</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../bio.html" class="button">Bio</a></li>
							<li><a href="../projects.html" class="button">Projects</a></li>
							<!-- li>
								<a href="#" class="icon fa-angle-down">Layouts</a>
								<ul>
									<li><a href="generic.html">Generic</a></li>
									<li><a href="contact.html">Contact</a></li>
									<li><a href="elements.html">Elements</a></li>
									<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option One</a></li>
											<li><a href="#">Option Two</a></li>
											<li><a href="#">Option Three</a></li>
											<li><a href="#">Option Four</a></li>
										</ul>
									</li>
								</ul>
							</li -->
						</ul>
					</nav>
				</header>
<h1>An Analysis of Twitter Brand Sentiment and Topic Modeling</h1>
<p>
*Warning* - The text analyzed in this article has not been cleaned for profanity. This is an analysis of 17 days of tweets for brands Uber and Lyft.
<style>.column {
  float: left;
  width: 33.3%;
}
/* two image containers (use 25% for four, and 50% for two, etc) */
/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img {
   width: 99%;
   height: auto;
 }
@media screen and (max-width: 1000px) {
  .column {
    width: 100%;
  }
}
</style>
<div class="row">
<div class="column">
      <img src="/images/lyft_wordcloud.png" alt="lyft_wordcloud" class="img">
    </div>
<div class="column">
      <img src="/images/uber_wordcloud.png" alt="uber_wordcloud" class="img">
    </div>
</div>
<h3>Background</h3>
While exploring metrics that may serve as useful in improving Mixed Media Model predictiveness, 
I decided to try to quantify brand health as an estimate of social sentiment, i.e. tweets about 
brands scored for how positively or negatively tinged they may be. With sentimentized tweets, I 
could then estimate how the average sentiment changes over time, estimate the average sentiment of
tweets including certain key words, and recommend actionable next steps to clients, related to 
affecting their brand messaging through advertising, or improving operational aspects of the business 
where failures drive complaints and drive away customers.<br>
<h3>The Data</h3>
The data used here is optained from the standard Twitter developer search API. <br><br>

A Note about accessing the Twitter standard API:<br>
In order to access this API, one must apply for a Twitter developer account using an existing Twitter 
account, create an app and record for Twitter what you will use developer access to the API endpoints for, and 
create a dev environment through the Twitter developer Dev Environments menu. Once your twitter app is 
created, you access your access keys and tokens for authenticating and making requests to the API endpoint. 
This post does not guide the reader through accessing the Twitter Standard API, but an example of authenticating 
is available in the code samples below.
<br><br>
The API will return a JSON of tweet information including meta data, related to the tweet and the user. 
The JSON string is to be converted to a Python dictionary for manipulation, and certain meta data is 
recorded along with the tweet itself. The Standard API returns a sample of 100 tweets daily with 
recency of up to 10 days, so this analysis is conducted against a sample of tweets and not a 
comprehensive record of tweets. The brands I examine here are ride-sharing competitors Lyft
and Uber.
<h3>Approach</h3>
This analysis primarily concerns data collection and deriving sentiment, topic classification, evaluating sentiment over time and generating word cloud reports as follows:
<p><blockquote><pre>
Data Collection:
	1. One-hundered tweets per day, for ten days and for each brand are accessed via the API endpoint.
	2. Tweets are cleaned for stopwords and meaningless punctuation.
	3. Tweets are fed to a VADER sentimentizer and negative, positive, neutral and compound scores are recorded with the tweet records

Word Cloud Analysis:
	1. Unique words for each brand are recorded with frequency of appearances in samples, and average tweet sentiment given appearance. 
	2. Average word sentiment is mapped to an RGB value such that negative compound scores are more red and positive scores are more green.
	3. Plot a conditionally colored word cloud and examine.
	
Topic Classification:
	1. A corpus of tweets are pulled from the API and labeled at collection as one of several topics that may classify the Uber and Lyft Tweets.
	2. A Naive Bayes Classifier is trained on this corpus to predict the topic of Uber and Lyft Tweets.
	3. Tweets are inspected to evaluate predictiveness of labeling model.
	4. A Neural Network is designed to convolve over senteces and predict the label as an alternative to the Naive Bayes Method, and tweets are inspected for model predictiveness.
	5. A Neural Network is designed to predict the label from the inclusion of key words and tweets are inspected for model predictiveness.
	6. Term Frequency * Inverse Document Frequency is evaluated for tweets (TF IDF) analysis, to evaluate word importance in documents.
	7. Latent Dirichlet allocation (LDA) is performed on tweets in the corpus, weighted by importance infered from TF IDF analysis.
	
Brand Comparison over Time:
	1. Segmented Tweets are pivoted for daily average sentiment.
	2. Segmented comparisons are evaluated to target nuances in brand health.
	3. Evaluation is made overall and within each predicted topic derived from the strongest topic model.
	
</blockquote></pre></p>

<h3>Data Collection and Intro to VADER Sentimentizer</h3>
A sample of 100 tweets, a mixture of the most recent tweets and the most popular tweets, are collected each day for each brand using the
Twitter Standard Search API. It is important to remember to search for tweets using the parameter "tweet_mode=extended" in your query 
string to ensure all characters beyond number 140 are captured in your data. Another potentially tricky of pulling full tweets is that 
retweeted tweets will contain a truncated tweet for the main tweet text, but will contain the full tweet in the nested retweeted data 
within the JSON document. To account for this nuance, I use a try statement to read the tweet in the nested retweeted data of the 
document, otherwise the main tweet text is read:
<p><blockquote><pre>
for i in range(len(statuses)):
    try:
        text.append(statuses[i]['retweeted_status']['full_text'])
    except:
        text.append(statuses[i]['full_text'])
</pre></blockquote></p>
Tweets need to be cleaned for programatic readability. The first stage of my cleaning will involve replacing emojis and un-rendered emoji 
unicode with descriptions of the emojis. I have sourced these from <a href="https://github.com/wooorm/gemoji/blob/master/support.md">here</a> by copying the table into a
tab separated .txt file, and reading into python with pandas. I can then create a mapping from rendered emojis to decriptions, and another
from unicode to descriptions, and replace emojis in tweets with these descriptions: 
<p><blockquote><pre>
#Load Emoji Unicode TSV (Make sure TBL is saved in txt file as utf-8)
emojis = pd.read_csv('C://Users/jliv/Downloads/emojis.txt',sep = '\t', encoding = 'utf-8')
#Map of Unicode and Names
emoji_map = pd.DataFrame()
emoji_map['name'] = emojis['Name(s)']
emoji_map['code'] = emojis['Escaped Unicode']
#Map of Emojis and names
emoji_map1 = pd.DataFrame()
emoji_map1['name'] = emojis['Name(s)']
emoji_map1['Emoji'] = emojis['Emoji']
#Handle escape characters in unicode
codes = [] 
for i in list(emojis['Escaped Unicode']):
    x = i.replace("\\","\\")
    codes.append(x)
    

emojislist = emoji_map1['Emoji']
#Convert CSVs of mappings to dict mappings
emoji_map.index = codes
emoji_dict = emoji_map.to_dict()
emoji_dict = emoji_dict['name']

emoji_map1.index = emojislist
emoji_dict1 = emoji_map1.to_dict()
emoji_dict1 = emoji_dict1['name']

#Replace tweet emojis and unicode with descriptions of characters
emoji_clean = []
for i in text:
    x = i
    for k,v in emoji_dict1.items():
        x = x.replace(k, v)
    for k,v in emoji_dict.items():
        x = x.replace(k, v)
    emoji_clean.append(x)
</pre></blockquote></p>
The next step in my cleaning exercise is to use regular expressions to recognize many website URLs that may
be present in tweets and remove these from the text. I search for 6 common URL patterns. I also remove 
largely non-sentimentized punctuation in the same phase of cleaning (e.g. commas, colons, parentheses):
<p><blockquote><pre>
tweetvector_clean = []
for i in emoji_clean:
    x = re.sub(r"^(http:\/\/www\.|https:\/\/www\.|http:\/\/|https:\/\/)?[a-z0-9]+([\-\.]{1}[a-z0-9]+)*\.[a-z]{2,5}(:[0-9]{1,5})?(\/.*)?$"," ", i)
    x = re.sub(r"htt\S+"," ", x) 
    x = re.sub(r"pic.twit\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"@\S+"," ", x)
    x = re.sub(r"\xa0"," ", x)
    x = re.sub(r"\\u\S+"," ", x)
    x = x.replace('#',' ')
    x = x.replace('amp;','&')
    x = x.replace('gt;',' ')
    x = x.replace('\\n',' ')
    y = x.replace('$',' ')
    y = y.replace('(',' ')
    y = y.replace('–',' ')
    y = y.replace('‘',' ')
    y = y.replace('“',' ')
    y = y.replace('”',' ')
    y = y.replace('`',' ')
    y = y.replace(']',' ')
    y = y.replace('[',' ')
    y = y.replace(';',' ')
    y = y.replace(')',' ')
    y = y.replace('/',' ')
    y = y.replace('*',' ')
    y = y.replace(',',' ')
    y = y.replace('’','')
    y = y.replace('.','')
    y = y.replace('-',' ')
    y = y.replace("'",'')
    y = y.replace(':',' ')
    y = y.replace('@',' ')
    y = y.replace('!',' ')
    y = y.replace('…',' ')
    y = y.replace('?',' ')
    y = y.replace('>',' ')
    y = y.replace('&',' ')
    y = y.replace("\\","")
    y = y.replace("\\u2066","")
    tweetvector_clean.append(y)
</pre></blockquote></p>
The next process in this text cleaning exercise is to tokenize documents so meaningless stopwords can be evaluated and
removed. I use the python Natural Language Tool Kit (NLTK) for much of this textual analysis, and I evaluate tweet 
stop words using a stop word bank from NLTK. I also pass tweets through an NLTK tokenization function for tokenizing:
<p><blockquote><pre>
Import nltk

stopwords = nltk.corpus.stopwords  
stop_words = set(stopwords.words("english"))
word_tokenize = nltk.tokenize.word_tokenize

#Tokenize to remove stopwords
tweetvector_tokenized = []
for i in tweetvector_clean:
    x = word_tokenize(i)
    tweetvector_tokenized.append(x)
tweetvector_stopped = []
for i in tweetvector_tokenized:
    newstatement = [j for j in i if j not in stop_words]
    tweetvector_stopped.append(newstatement)

#Rejoin tweets minus stopwords
final_tweets = []
for i in tweetvector_stopped:
    x = " ".join(i)
    final_tweets.append(x)
</pre></blockquote></p>
With tweets cleaned of meaningless words and symbols, text can then be fed to a sentimentizer to attempt to quantify 
the sentiment of the statement. I use the VADER sentimentizer through the Natural Language Tool Kit. A detailed 
account of VADER's development can be found <a href="http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf">here</a>. Vader stands for Valence Aware Dictionary for 
sEntiment Reasoning, and is primarily focused on creating a sentiment lexicon where words are labeled as positive or 
negatively connotated when used semantically. The developed lexicon is atuned for evaluating social media statements, and 
word Valence scores are point estimates derived from crowd sourced sentiment assessments (Mechanical Turk). Tweets 
can be evaluated for the percentage of words classified as positive, negative or neutral, and the normalized sum of word 
valences can estimate the compound sentiment of the tweet. Accessing VADER sentiment scoring is simple with NLTK:
<p><blockquote><pre>
sid = nltk.sentiment.vader.SentimentIntensityAnalyzer()
compound = []
neutral = []
negative = []
positive = []
for i in final_tweets:
    ss = sid.polarity_scores(i)
    comp = ss['compound']
    neg = ss['neg']
    neu = ss['neu']
    pos = ss['pos']
    compound.append(comp)
    neutral.append(neu)
    negative.append(neg)
    positive.append(pos)
</pre></blockquote></p>
Finally, with sentimentized tweets, I can evaluate the stems of words so that words of like-stems and lemmas can be 
analyzed together (e.g. Say, Said, Says are all lemmatized to Say). This is largely necessary for LDA analysis, but
also limits the scope of the wordclouding exercise to being more manageable. I do so using stemming and lemmatizing
from NLTK. A note about the code sample, final_tweets2 is a version of data final_tweets with terms 'uber' and 'lyft'
(exceedingly common) removed so these are not considered in topic models or word clouds:
<p><blockquote><pre>
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from nltk.stem.porter import *

import nltk
nltk.download('wordnet')
stemmer = SnowballStemmer('english')

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
    

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result
    
processed_docs = docdf['final_tweets2'].map(preprocess)
merged = []
for i in processed_docs:
    merged.append(" ".join(i))
docdf['final_tweets2_stemmed'] = merged
</pre></blockquote></p>
With tweets now associated with sentiment scores, I can focus on answering a couple of interesting questions:<br>

1. What keywords are associated with each brand, and how polarizing are the connotations?<br>

2. How does sentiment compare between the brands on average and over time?
<h3>Word Clouding and Comparing</h3>
When developing a word cloud, I am interested in size representing the frequency of use and color representing the 
polarity of the word when used. I don't simply want to color words by their crowd sourced valence scores. Rather, for 
each word, I want to evaluate the average compound sentiment of tweets that used the word, and color-code by that 
standard. With cleaned tweets mapped to compound scores following VADER sentimentizing, I can re-tokenize tweets, 
map words the tweet compound scores where words appear, then use a pivot table to list distinct words by frequency 
of occurance and average tweet compound sentiment when used:
<p><blockquote><pre>
tweetlist = list(tweetdf['final_tweets2'])

tokenized = []
compound = []
negative = []
neutral = []
positive = []
date = []
for i in range(len(tweetlist)):
    tokenized.append(tweetlist[i].split())
    compound.append(tweetdf['compound'][i])
    negative.append(tweetdf['negative'][i])
    neutral.append(tweetdf['neutral'][i])
    positive.append(tweetdf['positive'][i])
    date.append(tweetdf['date'][i])
    
words = []
compound2 = []
negative2 = []
neutral2 = []
positive2 = []
date2 = []
for i in range(len(tokenized)):
    for j in tokenized[i]:
        words.append(j.lower())
        compound2.append(compound[i])
        negative2.append(negative[i])
        neutral2.append(neutral[i])
        positive2.append(positive[i])
        date2.append(date[i])
        
wordsdf = pd.DataFrame()
wordsdf['date'] = date2
wordsdf['words'] = words
wordsdf['compound'] = compound2
wordsdf['negative'] = negative2
wordsdf['neutral'] = neutral2
wordsdf['positive'] = positive2

#DFs of unique words with average score when used
wordssent = pd.pivot_table(data = wordsdf, values = ['compound','negative','neutral','positive'], index = ['words'], aggfunc = 'mean')
wordscount = pd.pivot_table(data = wordsdf, values = ['compound'], index = ['words'], aggfunc = 'count')
wordssent['count'] = wordscount['compound']

#Sorted by Use
wordssent.sort_values(by = 'count', ascending = False, inplace = True)

wordssent.reset_index(drop = False, inplace = True)
 
</pre></blockquote></p>
I am implementing a wordcloud with the derived information using the python package wordcloud. The package contains an example function 
for mapping a single color to groups of words that should be that exact color, so I need to transform my list of compound scores into 
an (R,G,B) string, and I do so formulaically from the average compound score of each word, and the minimum and maximum average compound 
scores:
<div>
      <img src="/images/conditional_formating.png" alt="formatting">
    </div>
<p><blockquote><pre>
#List Unique Words and Colors derived from Compound score together
UW = []
color = []
for i,j in zip(list(wordssent[wordssent['count']>3]['words']),list(wordssent[wordssent['count']>3]['compound'])):
    UW.append(i)
    color.append('rgb('+str(int(255*(1- (j-compmin)/(compmax-compmin))))+','+str(int(155*(j-compmin)/(compmax-compmin)))+', 0)')

#For distinct colors, map a list of all words to the rgb() key in a dict
colorset = list(set(color))
color_to_words = {}
for i in colorset:
    words_by_color = []
    for j in range(len(UW)):
        if color[j] == i:
            words_by_color.append(UW[j])
        else:
            pass
    color_to_words[i] = words_by_color
	
</pre></blockquote></p>
The wordcloud package github provides examples for applying colors selectively to words in a number of ways. Here I use the simple 
grouped color function class to create a word: color mapping from the color_to_words mapping already created. This function simply
makes values in each value list within the color_to_words hash table the key for mapping back to the key value from the color_to_words 
hash table. The resulting dict is used for recoloring words within the wordcloud package:
<p><blockquote><pre>
class SimpleGroupedColorFunc(object):
    """Create a color function object which assigns EXACT colors
       to certain words based on the color to words mapping
       Parameters
       ----------
       color_to_words : dict(str -> list(str))
         A dictionary that maps a color to the list of words.
       default_color : str
         Color that will be assigned to a word that's not a member
         of any value from color_to_words.
    """

    def __init__(self, color_to_words, default_color):
        self.word_to_color = {word: color
                              for (color, words) in color_to_words.items()
                              for word in words}

        self.default_color = default_color

    def __call__(self, word, **kwargs):
        return self.word_to_color.get(word, self.default_color)
		
grouped_color_func = SimpleGroupedColorFunc(color_to_words, 'grey')


wordcloud = WordCloud(collocations = False,width = 800, height = 500,background_color = "black").generate(text)
wordcloud.recolor(color_func=grouped_color_func)
# Matplotlib to display the cloud
import matplotlib.pyplot as plt
plt.figure( figsize=(8,6) )
plt.imshow(wordcloud)
plt.axis("off")
plt.title("Lyft Word Cloud")
</pre></blockquote></p>
</style>
<div class="row">
<div class="column">
      <img src="/images/lyft_wordcloud.png" alt="lyft_wordcloud" class="img">
    </div>
<div class="column">
      <img src="/images/uber_wordcloud.png" alt="uber_wordcloud" class="img">
    </div>
</div>
Only lemmas with more than 30 appearances in each brand's word corpus are shown in these word clouds. For comparative purposes, 
the compmin and compmax are taken to be -1 and 1 for both brands, normalizing all data against the same scale. The resulting 
word clouds for each brand reveal positive connotations around Lyft's free ride offers and promotional codes, and positive 
connotations around use in Chicago and the idea of warming (this data was collected during a deep freeze, during which, 
Lyft offered free rides to transport people to warming shelters in Chicago). I have always used these word clouds as decent 
indicators of what aspects of a brand need improvement. For example, both brands are associated with negative contexts for 
drivers and driving. A key difference is these stemmed lemmas are largely more negative for Uber, as the Uber cloud is much 
less green (less positive) than the Lyft cloud. Another observation of note is strong negativity for Lyft relating to the 
words wage and worker, and an investigation shows Lyft's resistance to new minimum wage legislation in New York City has 
been poorly received. Much of the green-ness of the Lyft cloud, though, is attributable to promotional tweets, or informative 
tweets about a positive service Lyft offered during the limited collection window. 
<h3>Naive Bayes Topic Modeling and Summary Analytics</h3>
Understanding the topics tweets relate to is important for targeting areas for brand-health improvement. In order to guess the 
topics of tweets in an automated way, I use a general strategy of building a labeled collection of tweets through the same API
pull and text cleaning process used to pull and clean Uber and Lyft tweets, training models against this corpus, then predicting 
the labels of Uber and Lyft tweets. The first topic model I use to predict labels is the Naive Bayes Classifier.<br><br>
A Naive Bayes Classifier can be constructed using known probabilities of document features, and those of user defined labels in 
training data. This method is an application of Bayes Theorem, with the "Naive" assumption that features are indepentent of each 
other. Using this model, one might define the probability of some Label_a for a document with features X as:
<div >
      <img src="/images/naive_bayes_classifier.png" alt="NBC" class="jayimage">
    </div>
But because p(X) is a data dependent constant for all potential labels, only the numerator needs to be evaluated for each candidate 
label for a document, and the largest result is selected as the document label. <br><br>
Here is a simple example to conceptualize predicting labels with this model:
<p><blockquote><pre>
Consider 4 Sentences and labels:
<table>
<tr><th>Sentence</th><th>Stemmed/Cleaned</th><th>Label</th></tr>
<tr><td>He Is Lazy</td><td>He Lazy</td><td>Cat</td></tr>
<tr><td>Give Him Treats</td><td>Give He Treat</td><td>Dog</td></tr>
<tr><td>The Dog Barked</td><td>Dog Bark</td><td>Dog</td></tr>
<tr><td>The Cat Meowed</td><td>Cat Meow</td><td>Cat</td></tr>
</table>
With the above as training data, how is this new sentence classified:
Meows Are Cute

The new sentence can be cleaned:
Meow Cute

Features for consideration include words in the training corpus:
features = {He, Lazy, Give, Treat, Dog, Bark, Cat, Meow}

Evaluating p(Dog) for 'Meow Cute':
	p(Label): p(Dog) = .5  (evident in training data)
	p(X|Label): p(Meow|Dog) = 0 (from training data, Note that only Meow is a feature extracted from Training, Cute is not)
	p(Label_a|X) ~ p(Label)*p(X|Label): p(Dog)*p(Meow|Dog) = .5*0 = 0
	
Evaluating p(Cat) for 'Meow Cute':
	p(Label): p(Cat) = .5  
	p(X|Label): p(Meow|Cat) = .5 
	p(Label_a|X) ~ p(Label)*p(X|Label): p(Cat)*p(Meow|Cat) = .5*.5 = .25
	
p(Cat) > p(Dog), by the maximum a posteriori (MAP) rule, Cat is the selected label.
</pre></blockquote></p>
The Naive Bayes Classifier can be imported from the NLTK package. My strategy for applying the package is to collect the 
cleaned corpus of unique words in the training tweet corpus, limit the number of features to consider to the N most common 
and meaningful words, and evaluate te likelihood of candidate labels against that feature vector. Here I limit the feature 
vector to the most common 300 words and words within the absolute frequency range within the corpus of 20 - 30 (after 
removing stop words). Here is an example of implementing the NLTK Naive Bayes Classifier:
<p><blockquote><pre>
#Function to state True/False if a feature is in a document
def document_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words)
    return features

#Corpus DF csv contains cleaned 'final_tweets' and labels, previously loaded as pandas DF
corpusdf['count'] = 1
corpusdf2 = pd.pivot_table(data = corpusdf, index = ['final_tweets','label'], values = ['count'], aggfunc = 'sum')
corpusdf2.reset_index(inplace=True, drop = False)
corpusdf2['final_tweets'] = corpusdf2['final_tweets'].fillna(" ")
rand_tweets_labels = list(corpusdf2['label'])
unique_tweets = list(corpusdf2['final_tweets'])

#Tokenize Tweets
tweet_doc = []
for i in unique_tweets:
    x = i.lower()
    tweet_doc.append(x.split())
   
#All Words
unique_words = []
for i in unique_tweets:
    x = i.split()
    for j in x:
        unique_words.append(j.lower())   
		
#Unique Words by Count 
UW = pd.DataFrame()
UW['unique_words'] = unique_words
UW['count'] = 1
UW_piv = pd.pivot_table(data = UW, values = 'count', index = 'unique_words', aggfunc = 'sum')
UW_piv = UW_piv.sort_values(by = 'count', ascending = False)
unique_words2 = list(UW_piv.index)
#Drop a couple of stragler stop words from cleaning
dropping = ['i','the','``',"''"]
unique_words3 = [i for i in unique_words2 if i not in dropping]
word_features =unique_words3[:300]

ww = UW_piv.copy()

ww.reset_index(inplace= True)
ww = list(ww[(ww['count'] >= 20)&(ww['count'] <= 30)]['unique_words'])
word_features = list(word_features)+ww

#Create a list of tuples containing document features and labels
tweet_featset = [(document_features(d), c) for (d,c) in zip(tweet_doc,rand_tweets_labels)]   
   
#Train a naive bayes classifier   
train_set, test_set = tweet_featset[:len(tweet_featset)], tweet_featset[:len(tweet_featset)]
classifier = nltk.NaiveBayesClassifier.train(train_set)
classifier.show_most_informative_features(30)
nltk.classify.accuracy(classifier, test_set)

		Printed:
Most Informative Features
         contains(promo) = True           promot : financ =   1721.2 : 1.0
          contains(code) = True           promot : news   =   1246.9 : 1.0
    contains(complaints) = True           servic : news   =    692.8 : 1.0
      contains(customer) = True           servic : news   =    633.8 : 1.0
           contains(ipo) = True           financ : news   =    466.0 : 1.0
      contains(startups) = True           financ : servic =    427.0 : 1.0
           contains(fox) = True             news : financ =    338.7 : 1.0
          contains(news) = True             news : promot =    184.7 : 1.0
       contains(service) = True           servic : news   =    165.7 : 1.0
      contains(breaking) = True             news : servic =    141.4 : 1.0
        contains(driver) = True           servic : financ =    139.3 : 1.0
     contains(headlines) = True             news : financ =    122.3 : 1.0
        contains(market) = True           financ : servic =    118.1 : 1.0
       contains(startup) = True           financ : news   =    108.4 : 1.0
       contains(tickets) = True           promot : news   =    102.1 : 1.0
           contains(cnn) = True             news : financ =    101.2 : 1.0
          contains(save) = True           promot : financ =     95.8 : 1.0
          contains(code) = False            news : promot =     93.4 : 1.0
         contains(stock) = True           financ : news   =     89.9 : 1.0
          contains(uber) = True           servic : news   =     87.1 : 1.0
        contains(stocks) = True           financ : news   =     84.6 : 1.0
         contains(enter) = True           promot : financ =     69.1 : 1.0
         contains(trump) = True             news : servic =     66.8 : 1.0
          contains(tech) = True           financ : news   =     58.4 : 1.0
           contains(buy) = True           promot : news   =     54.6 : 1.0
           contains(use) = True           promot : financ =     52.5 : 1.0
            contains(50) = True           promot : news   =     50.6 : 1.0
           contains(iot) = True           financ : news   =     48.9 : 1.0
             contains(|) = True             news : servic =     48.6 : 1.0
            contains(ai) = True           financ : news   =     46.5 : 1.0
0.9594858257775737
</pre></blockquote></p>
The classifier describes the training data with 96% accuracy. Similarly, when I randomize the tweet order and 
hold out 25% for predicting the label, the classifier predicts with 95.4% accuracy on unseen tweets. The built 
classifier is then used to label Lyft and Uber tweet topics:
<p><blockquote><pre>
#Tokenize UBer/Lyft Tweets for Classifying
unique_tweets = tweetdf['final_tweets']
tweets_split = []
for i in unique_tweets:
    tweets_split.append(i.split())

#Predict Labels of Documents and report p(Label) for selected labels, save to df
new_labels = []
probs = []
for i in tweets_split:
    test_features = [(document_features(i), 'test')]
    new_labels.append(classifier.classify(test_features[0][0]))
    dist = classifier.prob_classify(test_features[0][0])
    probs.append(dist.prob(classifier.classify(test_features[0][0])))

tweetdf['NB_label'] = new_labels
tweetdf['NB_label_prob'] = probs
</pre></blockquote></p>
Evaluating the average compound tweet sentiment by tweet topic may yield some interesting findings. Particularly, 
much of Lyft's positive sentiment may be inflated by a surge of tweets related to promotional codes and free rides. 
One would find that relatively few Uber tweets have been classified as promotional to begin with:
<div >
      <img src="/images/Topics_Sentiment_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
Over this limited time window, daily average sentiment is noisy for the two brands, but Lyft appears broadly 
healthier (For longer time windows I like to look at rolling averages and how these may diverge). Do not forget 
that this may be related to being more promotional overall:
<div >
      <img src="/images/Time_Sentiment_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
In terms of financial tweets, Uber and Lyft demonstrate highly correlated sentiment, which passes the logic test 
that market news about the ride share space is more likely to reference both brands and affect both businesses 
than service complaints or promotions relating to one brand:
<div >
      <img src="/images/Time_Sentiment_Financial_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
Lyft may demonstrate some positivity diverging from Uber in news related tweets. This may motivate the data scientist 
to perform a keyword analysis on these tweets in particular to understand the major differences. Another wordcloud may 
serve well for this purpose (I am not generating the new word clouds at this time):
<div >
      <img src="/images/Time_Sentiment_News_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
Uber registers far fewer promotional tweets as previously stated, but the Lyft promotional tweets provide a strong 
baseline of positive sentmient to the tweet corpus:
<div >
      <img src="/images/Time_Sentiment_Promotional_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
Uber service tweets seem to have a downward inflection during this period, which seems to drive the larger decline 
in Uber's brand sentiment during the period. I would also analyze keywords from these tweets to understand the 
specifics of potential service failures:
<div >
      <img src="/images/Time_Sentiment_Service_NBC.png" alt="NBC" class="jayimage" width="500">
    </div>
To verify these findings or discover any potential learnings from a second model that the Naive Bayes Classifier could 
not distinguish, I also construct a neural network to learn the Uber and Lyft tweet topics from the corpus of collected 
tweets. I do so to verify if meaningful classification improvements can be made when stepping away from the Naive 
assumption of feature independence. 
<h3>Neural Network Topic Modeling from Doc Features</h3>
I approach Neural Network modeling using Tensorflow to develop a simple, deep feature model that considers the same inputs
as the Naive Bayes Model, and to develop a network designed to convolve over embedded sentences before training with some 
deepness. I do so for comparative purposes with the Naive Bayes method. 
<div >
      <img src="/images/NN1config.png" alt="NBC" class="jayimage" width="500">
    </div>
This example network represents one tested network against the 787 tweet features evaluated in the Naive Bayes analysis. 
This network uses Rectivied Linear Unit activation in the hidden layers and sigmoid activation in the output layer for 
predicting the four labels. While the network describes training data better than the Naive Bayes analysis (96.7%), this 
network only predicts unseen labels with 89% accuracy. 
<div >
      <img src="/images/CNN2.png" alt="NBC" class="jayimage" width="500">
    </div>
This second network design consideres the 2500 most common words from the training tweet corpus, arranges these binaries 
into an ordered (L -> R, U -> D by word frequency) 50X50 matrix and convolves over the matrix in a similar fashion to an 
image analysis, flattens the output of convolution to one dimension, and outputs that result to two hidden layers of 25 
Relu neurons. Finally the output of the hidden layers is fed to the output layer of 4 Sigmoid neurons. This network 
described training data with 98.4% accuracy, but only predicted with 91.6% accuracy. To this point, the Naive Bayes 
Classifier is the strongest topic model.<br><br>
I tested another network of the same design with different preprocessing to the documents. Rather than order features in 
the 50X50 information matrix by descending frequency, I wanted to represent words on the 50X50 matrix as being spacially 
closer to words that are lexiconically similar. To do so, I implemented a Word2Vec model to represent similarly used words 
as spatially close, performed a principle component analysis on the high dimensional Word2Vec numeric representation of 
each word, to obtain two principle components, then rescaled these components to points in the 50X50 information matrix 
space, recording a 1 for values in the matrix where a word is mapped, and a zero where no word is mapped. The Word2Vec 
model is simple to implement from the Gensim package:
<p><blockquote><pre>
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
sentences = list(corpusdf['processed_docs'])
#Limit sentences to 2500 most common words (contained in list WL)
sentences2 = []
for i in sentences:
    x = [j for j in i if j in WL]
    sentences2.append(x)

#Fit Word2Vec Model
wvmodel = Word2Vec(sentences2, min_count=1)
X = wvmodel[wvmodel.wv.vocab]
	
pca = PCA(n_components=2)
result = pca.fit_transform(X)

words = list(wvmodel.wv.vocab)

#Map words in wvmodel vocabulary to Word2Vec PCA coordinates
wordmap = {}
for i in range(len(words)):
    wordmap[words[i]] = list(result[i])
	
#Use Min and Max x and y of Word2Vec word model to scale to a point in (0:49,0:49)
 minx = min(result[:, 0])
maxx = max(result[:, 0])
miny = min(result[:, 1])
maxy = max(result[:, 1])
docmats = []

for i in list(tdf1['processed_docs']):
    stc = np.array(np.zeros(2500)).reshape(50,50)
    doc = np.array([wordmap[j] for j in i if j in wordmap.keys()])
    try:
        doc[:,0] = np.round((doc[:,0]-minx)*49/(maxx-minx))
        doc[:,1] = np.round((doc[:,1]-miny)*49/(maxy-miny))
        for k in doc:
            stc[int(k[0])][int(k[1])] = 1
        docmats.append(stc)
    except:
        docmats.append(stc)
        pass

docmats = np.array(docmats)
</pre></blockquote></p>
The resulting 'docmats' of tweets can be fed to convolutional or pooling layers.<br><br>
This model described training data with 99% accuracy, and labeled holdout data with 93.5% accuracy, an improvement over 
other designed networks thusfar, but still not as strong in classification as the Naive Bayes Classifier. <br><br>
Code Snippets Below<br><br>
<!--style>.indented {
  padding-left: 50pt;
  padding-right: 50pt;
}</style>
<style>.tab {
  padding-left: 50pt;
}</style>
<div class="indented"-->
<p><blockquote><pre>

</pre></blockquote></p>
<h3>Outcomes and Learnings</h3>
 
<h3>Summary and Takeaways</h3>
<br><br>

</p>
<li><a href="/projects/hill_fit.html">See the Script</a></li>
</body>
</html>