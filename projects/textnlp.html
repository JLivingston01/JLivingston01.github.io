<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131669567-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131669567-1');
</script>
<!-- End google Code -->
<!-- Matomo -->
<script type="text/javascript">
  var _paq = _paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
  _paq.push(["setCookieDomain", "*.jlivingston01.github.io"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://jlivingston01.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src='//cdn.matomo.cloud/jlivingston01.matomo.cloud/piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->
<!-- Matomo Image Tracker-->
<noscript>
<img src="https://jlivingston01.matomo.cloud/piwik.php?idsite=1&amp;rec=1&amp;action_name=machinelearning" style="border:0" alt="" />
</noscript>
<!-- End Matomo -->
</head>
<body>
<header id="header" class="alt">
					<h1><a href="../index.html">JLIV's Github Site</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../bio.html" class="button">Bio</a></li>
							<li><a href="../projects.html" class="button">Projects</a></li>
							<!-- li>
								<a href="#" class="icon fa-angle-down">Layouts</a>
								<ul>
									<li><a href="generic.html">Generic</a></li>
									<li><a href="contact.html">Contact</a></li>
									<li><a href="elements.html">Elements</a></li>
									<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option One</a></li>
											<li><a href="#">Option Two</a></li>
											<li><a href="#">Option Three</a></li>
											<li><a href="#">Option Four</a></li>
										</ul>
									</li>
								</ul>
							</li -->
						</ul>
					</nav>
				</header>
<h1>An Analysis of Twitter Brand Sentiment and Topic Modeling</h1>
<p>
*Warning* - The text analyzed in this article has not been cleaned for profanity. This is an analysis of 15 days of tweets for brands Uber and Lyft.
<style>.column {
  float: left;
  width: 33.3%;
}
/* two image containers (use 25% for four, and 50% for two, etc) */
/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img {
   width: 99%;
   height: auto;
 }
@media screen and (max-width: 1000px) {
  .column {
    width: 100%;
  }
}
</style>
<div class="row">
<div class="column">
      <img src="/images/lyft_wordcloud.png" alt="lyft_wordcloud" class="img">
    </div>
<div class="column">
      <img src="/images/uber_wordcloud.png" alt="uber_wordcloud" class="img">
    </div>
</div>
<h3>Background</h3>
While exploring metrics that may serve as useful in improving Mixed Media Model predictiveness, 
I decided to try to quantify brand health as an estimate of social sentiment, i.e. tweets about 
brands scored for how positively or negatively tinged they may be. With sentimentized tweets, I 
could then estimate how the average sentiment changes over time, estimate the average sentiment of
tweets including certain key words, and recommend actionable next steps to clients, related to 
affecting their brand messaging through advertising, or improving operational aspects of the business 
where failures drive complaints and drive away customers.<br>
<h3>The Data</h3>
The data used here is optained from the standard Twitter developer search API. <br><br>

A Note about accessing the Twitter standard API:<br>
In order to access this API, one must apply for a Twitter developer account using an existing Twitter 
account, create an app and record for Twitter what you will use developer access to the API endpoints for, and 
create a dev environment through the Twitter developer Dev Environments menu. Once your twitter app is 
created, you access your access keys and tokens for authenticating and making requests to the API endpoint. 
This post does not guide the reader through accessing the Twitter Standard API, but an example of authenticating 
is available in the code samples below.
<br><br>
The API will return a JSON of tweet information including meta data, related to the tweet and the user. 
The JSON string is to be converted to a Python dictionary for manipulation, and certain meta data is 
recorded along with the tweet itself. The Standard API returns a sample of 100 tweets daily with 
recency of up to 10 days, so this analysis is conducted against a sample of tweets and not a 
comprehensive record of tweets. The brands I examine here are ride-sharing competitors Lyft
and Uber.
<h3>Approach</h3>
This analysis primarily concerns data collection and deriving sentiment, topic classification, evaluating sentiment over time and generating word cloud reports as follows:
<p><blockquote><pre>
Data Collection:
	1. One-hundered tweets per day, for ten days and for each brand are accessed via the API endpoint.
	2. Tweets are cleaned for stopwords and meaningless punctuation.
	3. Tweets are fed to a VADER sentimentizer and negative, positive, neutral and compound scores are recorded with the tweet records

Topic Classification:
	1. Term Frequency * Inverse Document Frequency is evaluated for tweets (TF IDF) analysis, to group tweets into topics
	2. Latent Dirichlet allocation (LDA) is performed on tweets in the corpus, weighted by importance infered from TF IDF analysis.
	3. A Naive Bayes Classifier is an alternative method for labeling document topics, used for comparative purposes.
	
Brand Comparison over Time:
	1. Segmented Tweets are pivoted for daily average sentiment.
	2. Segmented comparisons are evaluated to target nuances in brand health.
	
Word Cloud Analysis:
	1. Unique words for each brand are recorded with frequency of appearances in samples, and average tweet sentiment given appearance. 
	2. Average word sentiment is mapped to an RGB value such that negative compound scores are more red and positive scores are more green.
	3. Plot a conditionally colored word cloud and examine.
	
</blockquote></pre></p>

<h3>Data Collection and Intro to VADER Sentimentizer</h3>
A sample of 100 tweets, a mixture of the most recent tweets and the most popular tweets, are collected each day for each brand using the
Twitter Standard Search API. It is important to remember to search for tweets using the parameter "tweet_mode=extended" in your query 
string to ensure all characters beyond number 140 are captured in your data. Another potentially tricky of pulling full tweets is that 
retweeted tweets will contain a truncated tweet for the main tweet text, but will contain the full tweet in the nested retweeted data 
within the JSON document. To account for this nuance, I use a try statement to read the tweet in the nested retweeted data of the 
document, otherwise the main tweet text is read:
<p><blockquote><pre>
for i in range(len(statuses)):
    try:
        text.append(statuses[i]['retweeted_status']['full_text'])
    except:
        text.append(statuses[i]['full_text'])
</pre></blockquote></p>
Tweets need to be cleaned for programatic readability. The first stage of my cleaning will involve replacing emojis and un-rendered emoji 
unicode with descriptions of the emojis. I have sourced these from <a href="https://github.com/wooorm/gemoji/blob/master/support.md">here</a> by copying the table into a
tab separated .txt file, and reading into python with pandas. I can then create a mapping from rendered emojis to decriptions, and another
from unicode to descriptions, and replace emojis in tweets with these descriptions: 
<p><blockquote><pre>
#Load Emoji Unicode TSV (Make sure TBL is saved in txt file as utf-8)
emojis = pd.read_csv('C://Users/jliv/Downloads/emojis.txt',sep = '\t', encoding = 'utf-8')
#Map of Unicode and Names
emoji_map = pd.DataFrame()
emoji_map['name'] = emojis['Name(s)']
emoji_map['code'] = emojis['Escaped Unicode']
#Map of Emojis and names
emoji_map1 = pd.DataFrame()
emoji_map1['name'] = emojis['Name(s)']
emoji_map1['Emoji'] = emojis['Emoji']
#Handle escape characters in unicode
codes = [] 
for i in list(emojis['Escaped Unicode']):
    x = i.replace("\\","\\")
    codes.append(x)
    

emojislist = emoji_map1['Emoji']
#Convert CSVs of mappings to dict mappings
emoji_map.index = codes
emoji_dict = emoji_map.to_dict()
emoji_dict = emoji_dict['name']

emoji_map1.index = emojislist
emoji_dict1 = emoji_map1.to_dict()
emoji_dict1 = emoji_dict1['name']

#Replace tweet emojis and unicode with descriptions of characters
emoji_clean = []
for i in text:
    x = i
    for k,v in emoji_dict1.items():
        x = x.replace(k, v)
    for k,v in emoji_dict.items():
        x = x.replace(k, v)
    emoji_clean.append(x)
</pre></blockquote></p>
The next step in my cleaning exercise is to use regular expressions to recognize many website URLs that may
be present in tweets and remove these from the text. I search for 6 common URL patterns. I also remove 
largely non-sentimentized punctuation in the same phase of cleaning (e.g. commas, colons, parentheses):
<p><blockquote><pre>
tweetvector_clean = []
for i in emoji_clean:
    x = re.sub(r"^(http:\/\/www\.|https:\/\/www\.|http:\/\/|https:\/\/)?[a-z0-9]+([\-\.]{1}[a-z0-9]+)*\.[a-z]{2,5}(:[0-9]{1,5})?(\/.*)?$"," ", i)
    x = re.sub(r"htt\S+"," ", x) 
    x = re.sub(r"pic.twit\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"@\S+"," ", x)
    x = re.sub(r"\xa0"," ", x)
    x = re.sub(r"\\u\S+"," ", x)
    x = x.replace('#',' ')
    x = x.replace('amp;','&')
    x = x.replace('gt;',' ')
    x = x.replace('\\n',' ')
    y = x.replace('$',' ')
    y = y.replace('(',' ')
    y = y.replace('–',' ')
    y = y.replace('‘',' ')
    y = y.replace('“',' ')
    y = y.replace('”',' ')
    y = y.replace('`',' ')
    y = y.replace(']',' ')
    y = y.replace('[',' ')
    y = y.replace(';',' ')
    y = y.replace(')',' ')
    y = y.replace('/',' ')
    y = y.replace('*',' ')
    y = y.replace(',',' ')
    y = y.replace('’','')
    y = y.replace('.','')
    y = y.replace('-',' ')
    y = y.replace("'",'')
    y = y.replace(':',' ')
    y = y.replace('@',' ')
    y = y.replace('!',' ')
    y = y.replace('…',' ')
    y = y.replace('?',' ')
    y = y.replace('>',' ')
    y = y.replace('&',' ')
    y = y.replace("\\","")
    y = y.replace("\\u2066","")
    tweetvector_clean.append(y)
</pre></blockquote></p>
The next process in this text cleaning exercise is to tokenize documents so meaningless stopwords can be evaluated and
removed. I use the python Natural Language Tool Kit (NLTK) for much of this textual analysis, and I evaluate tweet 
stop words using a stop word bank from NLTK. I also pass tweets through an NLTK tokenization function for tokenizing:
<p><blockquote><pre>
Import nltk

stopwords = nltk.corpus.stopwords  
stop_words = set(stopwords.words("english"))
word_tokenize = nltk.tokenize.word_tokenize

#Tokenize to remove stopwords
tweetvector_tokenized = []
for i in tweetvector_clean:
    x = word_tokenize(i)
    tweetvector_tokenized.append(x)
tweetvector_stopped = []
for i in tweetvector_tokenized:
    newstatement = [j for j in i if j not in stop_words]
    tweetvector_stopped.append(newstatement)

#Rejoin tweets minus stopwords
final_tweets = []
for i in tweetvector_stopped:
    x = " ".join(i)
    final_tweets.append(x)
</pre></blockquote></p>
With tweets cleaned of meaningless words and symbols, text can then be fed to a sentimentizer to attempt to quantify 
the sentiment of the statement. I use the VADER sentimentizer through the Natural Language Tool Kit. A detailed 
account of VADER's development can be found <a href="http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf">here</a>. Vader stands for Valence Aware Dictionary for 
sEntiment Reasoning, and is primarily focused on creating a sentiment lexicon where words are labeled as positive or 
negatively connotated when used semantically. The developed lexicon is atuned for evaluating social media statements, and 
word Valence scores are point estimates derived from crowd sourced sentiment assessments (Mechanical Turk). Tweets 
can be evaluated for the percentage of words classified as positive, negative or neutral, and the normalized sum of word 
valences can estimate the compound sentiment of the tweet. Accessing VADER sentiment scoring is simple with NLTK:
<p><blockquote><pre>
sid = nltk.sentiment.vader.SentimentIntensityAnalyzer()
compound = []
neutral = []
negative = []
positive = []
for i in final_tweets:
    ss = sid.polarity_scores(i)
    comp = ss['compound']
    neg = ss['neg']
    neu = ss['neu']
    pos = ss['pos']
    compound.append(comp)
    neutral.append(neu)
    negative.append(neg)
    positive.append(pos)
</pre></blockquote></p>
Finally, with sentimentized tweets, I can evaluate the stems of words so that words of like-stems and lemmas can be 
analyzed together (e.g. Say, Said, Says are all lemmatized to Say). This is largely necessary for LDA analysis, but
also limits the scope of the wordclouding exercise to being more manageable. I do so using stemming and lemmatizing
from NLTK. A note about the code sample, final_tweets2 is a version of data final_tweets with terms 'uber' and 'lyft'
(exceedingly common) removed so these are not considered in topic models or word clouds:
<p><blockquote><pre>
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from nltk.stem.porter import *

import nltk
nltk.download('wordnet')
stemmer = SnowballStemmer('english')

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
    

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result
    
processed_docs = docdf['final_tweets2'].map(preprocess)
merged = []
for i in processed_docs:
    merged.append(" ".join(i))
docdf['final_tweets2_stemmed'] = merged
</pre></blockquote></p>
With tweets now associated with sentiment scores, I can focus on answering a couple of interesting questions:<br>

1. What keywords are associated with each brand, and how polarizing are the connotations?<br>

2. How does sentiment compare between the brands on average and over time?
<h3>Word Clouding and Comparing</h3>
When developing a word cloud, I am interested in size representing the frequency of use and color representing the 
polarity of the word when used. I don't simply want to color words by their crowd sourced valence scores. Rather, for 
each word, I want to evaluate the average compound sentiment of tweets that used the word, and color-code by that 
standard. With cleaned tweets mapped to compound scores following VADER sentimentizing, I can re-tokenize tweets, 
map words the tweet compound scores where words appear, then use a pivot table to list distinct words by frequency 
of occurance and average tweet compound sentiment when used:
<p><blockquote><pre>
tweetlist = list(tweetdf['final_tweets2'])

tokenized = []
compound = []
negative = []
neutral = []
positive = []
date = []
for i in range(len(tweetlist)):
    tokenized.append(tweetlist[i].split())
    compound.append(tweetdf['compound'][i])
    negative.append(tweetdf['negative'][i])
    neutral.append(tweetdf['neutral'][i])
    positive.append(tweetdf['positive'][i])
    date.append(tweetdf['date'][i])
    
words = []
compound2 = []
negative2 = []
neutral2 = []
positive2 = []
date2 = []
for i in range(len(tokenized)):
    for j in tokenized[i]:
        words.append(j.lower())
        compound2.append(compound[i])
        negative2.append(negative[i])
        neutral2.append(neutral[i])
        positive2.append(positive[i])
        date2.append(date[i])
        
wordsdf = pd.DataFrame()
wordsdf['date'] = date2
wordsdf['words'] = words
wordsdf['compound'] = compound2
wordsdf['negative'] = negative2
wordsdf['neutral'] = neutral2
wordsdf['positive'] = positive2

#DFs of unique words with average score when used
wordssent = pd.pivot_table(data = wordsdf, values = ['compound','negative','neutral','positive'], index = ['words'], aggfunc = 'mean')
wordscount = pd.pivot_table(data = wordsdf, values = ['compound'], index = ['words'], aggfunc = 'count')
wordssent['count'] = wordscount['compound']

#Sorted by Use
wordssent.sort_values(by = 'count', ascending = False, inplace = True)

wordssent.reset_index(drop = False, inplace = True)
 
</pre></blockquote></p>
I am implementing a wordcloud with the derived information using the python package wordcloud. The package contains an example function 
for mapping a single color to groups of words that should be that exact color, so I need to transform my list of compound scores into 
an (R,G,B) string, and I do so formulaically from the average compound score of each word, and the minimum and maximum average compound 
scores:
<div>
      <img src="/images/conditional_formating.png" alt="formatting">
    </div>
<pre><blockquote><p>
#List Unique Words and Colors derived from Compound score together
UW = []
color = []
for i,j in zip(list(wordssent[wordssent['count']>3]['words']),list(wordssent[wordssent['count']>3]['compound'])):
    UW.append(i)
    color.append('rgb('+str(int(255*(1- (j-compmin)/(compmax-compmin))))+','+str(int(155*(j-compmin)/(compmax-compmin)))+', 0)')

#For distinct colors, map a list of all words to the rgb() key in a dict
colorset = list(set(color))
color_to_words = {}
for i in colorset:
    words_by_color = []
    for j in range(len(UW)):
        if color[j] == i:
            words_by_color.append(UW[j])
        else:
            pass
    color_to_words[i] = words_by_color
	
</pre></blockquote></p>
The wordcloud package github provides examples for applying colors selectively to words in a number of ways. Here I use the simple 
grouped color function class to create a word: color mapping from the color_to_words mapping already created. This function simply
makes values in each value list within the color_to_words hash table the key for mapping back to the key value from the color_to_words 
hash table. The resulting dict is used for recoloring words within the wordcloud package:
<pre><blockquote><p>
class SimpleGroupedColorFunc(object):
    """Create a color function object which assigns EXACT colors
       to certain words based on the color to words mapping
       Parameters
       ----------
       color_to_words : dict(str -> list(str))
         A dictionary that maps a color to the list of words.
       default_color : str
         Color that will be assigned to a word that's not a member
         of any value from color_to_words.
    """

    def __init__(self, color_to_words, default_color):
        self.word_to_color = {word: color
                              for (color, words) in color_to_words.items()
                              for word in words}

        self.default_color = default_color

    def __call__(self, word, **kwargs):
        return self.word_to_color.get(word, self.default_color)
		
grouped_color_func = SimpleGroupedColorFunc(color_to_words, 'grey')


wordcloud = WordCloud(collocations = False,width = 800, height = 500,background_color = "black").generate(text)
wordcloud.recolor(color_func=grouped_color_func)
# Matplotlib to display the cloud
import matplotlib.pyplot as plt
plt.figure( figsize=(8,6) )
plt.imshow(wordcloud)
plt.axis("off")
plt.title("Lyft Word Cloud")
</p></blockquote></pre>
</style>
<div class="row">
<div class="column">
      <img src="/images/lyft_wordcloud.png" alt="lyft_wordcloud" class="img">
    </div>
<div class="column">
      <img src="/images/uber_wordcloud.png" alt="uber_wordcloud" class="img">
    </div>
</div>
Only lemmas with more than 30 appearances in each brand's word corpus are shown in these word clouds. For comparative purposes, 
the compmin and compmax are taken to be -1 and 1 for both brands, normalizing all data against the same scale. The resulting 
word clouds for each brand reveal positive connotations around Lyft's free ride offers and promotional codes, and positive 
connotations around use in Chicago and the idea of warming (this data was collected during a deep freeze, during which, 
Lyft offered free rides to transport people to warming shelters in Chicago). I have always used these word clouds as decent 
indicators of what aspects of a brand need improvement. For example, both brands are associated with negative contexts for 
drivers and driving. A key difference is these stemmed lemmas are largely more negative for Uber, as the Uber cloud is much 
less green (less positive) than the Lyft cloud. Much of the green-ness of the Lyft cloud, though, is attributable to promotional 
tweets, or informative tweets about a positive service Lyft offered during the limited collection window. 
<h3>TF IDF and LDA for Top Classification</h3>
<br>

<h3>Code Snippets</h3>
Code Snippets Below<br><br>
<!--style>.indented {
  padding-left: 50pt;
  padding-right: 50pt;
}</style>
<style>.tab {
  padding-left: 50pt;
}</style>
<div class="indented"-->
<p><blockquote><pre>

</pre></blockquote></p>
<h3>Outcomes and Learnings</h3>
 
<h3>Summary and Takeaways</h3>
<br><br>

</p>
<li><a href="/projects/hill_fit.html">See the Script</a></li>
</body>
</html>