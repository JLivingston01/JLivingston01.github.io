<!DOCTYPE html>
<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-131669567-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-131669567-1');
</script>
<!-- End google Code -->
<!-- Matomo -->
<script type="text/javascript">
  var _paq = _paq || [];
  /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
  _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
  _paq.push(["setCookieDomain", "*.jlivingston01.github.io"]);
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://jlivingston01.matomo.cloud/";
    _paq.push(['setTrackerUrl', u+'piwik.php']);
    _paq.push(['setSiteId', '1']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.type='text/javascript'; g.async=true; g.defer=true; g.src='//cdn.matomo.cloud/jlivingston01.matomo.cloud/piwik.js'; s.parentNode.insertBefore(g,s);
  })();
</script>
<!-- End Matomo Code -->
<!-- Matomo Image Tracker-->
<noscript>
<img src="https://jlivingston01.matomo.cloud/piwik.php?idsite=1&amp;rec=1&amp;action_name=machinelearning" style="border:0" alt="" />
</noscript>
<!-- End Matomo -->
</head>
<body>
<header id="header" class="alt">
					<h1><a href="../index.html">JLIV's Github Site</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="../index.html">Home</a></li>
							<li><a href="../bio.html" class="button">Bio</a></li>
							<li><a href="../projects.html" class="button">Projects</a></li>
							<!-- li>
								<a href="#" class="icon fa-angle-down">Layouts</a>
								<ul>
									<li><a href="generic.html">Generic</a></li>
									<li><a href="contact.html">Contact</a></li>
									<li><a href="elements.html">Elements</a></li>
									<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option One</a></li>
											<li><a href="#">Option Two</a></li>
											<li><a href="#">Option Three</a></li>
											<li><a href="#">Option Four</a></li>
										</ul>
									</li>
								</ul>
							</li -->
						</ul>
					</nav>
				</header>
<h1>An Analysis of Twitter Brand Sentiment and Topic Modeling</h1>
<p>

<style>.column {
  float: left;
  width: 33.3%;
}
/* two image containers (use 25% for four, and 50% for two, etc) */
/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img {
   width: 99%;
   height: auto;
 }
@media screen and (max-width: 1000px) {
  .column {
    width: 100%;
  }
}
</style>
<div class="row">
<div class="column">
      <img src="/images/lyft_wordcloud.png" alt="lyft_wordcloud" class="img">
    </div>
<div class="column">
      <img src="/images/uber_wordcloud.png" alt="uber_wordcloud" class="img">
    </div>
</div>
<h3>Background</h3>
While exploring metrics that may serve as useful in improving Mixed Media Model predictiveness, 
I decided to try to quantify brand health as an estimate of social sentiment, i.e. tweets about 
brands scored for how positively or negatively tinged they may be. With sentimentized tweets, I 
could then estimate how the average sentiment changes over time, estimate the average sentiment of
tweets including certain key words, and recommend actionable next steps to clients, related to 
affecting their brand messaging through advertising, or improving operational aspects of the business 
where failures drive complaints and drive away customers.<br>
<h3>The Data</h3>
The data used here is optained from the standard Twitter developer search API. <br><br>

A Note about accessing the Twitter standard API:<br>
In order to access this API, one must apply for a Twitter developer account using an existing Twitter 
account, create an app and record for Twitter what you will use developer access to the API endpoints for, and 
create a dev environment through the Twitter developer Dev Environments menu. Once your twitter app is 
created, you access your access keys and tokens for authenticating and making requests to the API endpoint. 
This post does not guide the reader through accessing the Twitter Standard API, but an example of authenticating 
is available in the code samples below.
<br><br>
The API will return a JSON of tweet information including meta data, related to the tweet and the user. 
The JSON string is to be converted to a Python dictionary for manipulation, and certain meta data is 
recorded along with the tweet itself. The Standard API returns a sample of 100 tweets daily with 
recency of up to 10 days, so this analysis is conducted against a sample of tweets and not a 
comprehensive record of tweets. The brands I examine here are ride-sharing competitors Lyft
and Uber.
<h3>Approach</h3>
This analysis primarily concerns data collection and deriving sentiment, topic classification, evaluating sentiment over time and generating word cloud reports as follows:
<p><blockquote><pre>
Data Collection:
	1. One-hundered tweets per day, for ten days and for each brand are accessed via the API endpoint.
	2. Tweets are cleaned for stopwords and meaningless punctuation.
	3. Tweets are fed to a VADER sentimentizer and negative, positive, neutral and compound scores are recorded with the tweet records

Topic Classification:
	1. Term Frequency * Inverse Document Frequency is evaluated for tweets (TF IDF) analysis, to group tweets into topics
	2. Latent Dirichlet allocation (LDA) is performed on tweets in the corpus, weighted by importance infered from TF IDF analysis.
	3. A Naive Bayes Classifier is an alternative method for labeling document topics, used for comparative purposes.
	
Brand Comparison over Time:
	1. Segmented Tweets are pivoted for daily average sentiment.
	2. Segmented comparisons are evaluated to target nuances in brand health.
	
Word Cloud Analysis:
	1. Unique words for each brand are recorded with frequency of appearances in samples, and average tweet sentiment given appearance. 
	2. Average word sentiment is mapped to an RGB value such that negative compound scores are more red and positive scores are more green.
	3. Plot a conditionally colored word cloud and examine.
	
</blockquote></pre></p>

<h3>Data Collection and Intro to VADER Sentimentizer</h3>
A sample of 100 tweets, a mixture of the most recent tweets and the most popular tweets, are collected each day for each brand using the
Twitter Standard Search API. It is important to remember to search for tweets using the parameter "tweet_mode=extended" in your query 
string to ensure all characters beyond number 140 are captured in your data. Another potentially tricky of pulling full tweets is that 
retweeted tweets will contain a truncated tweet for the main tweet text, but will contain the full tweet in the nested retweeted data 
within the JSON document. To account for this nuance, I use a try statement to read the tweet in the nested retweeted data of the 
document, otherwise the main tweet text is read:
<p><blockquote><pre>
for i in range(len(statuses)):
    try:
        text.append(statuses[i]['retweeted_status']['full_text'])
    except:
        text.append(statuses[i]['full_text'])
</pre></blockquote></p>
Tweets need to be cleaned for programatic readability. The first stage of my cleaning will involve replacing emojis and un-rendered emoji 
unicode with descriptions of the emojis. I have sourced these from <a href="https://github.com/wooorm/gemoji/blob/master/support.md">here</a> by copying the table into a
tab separated .txt file, and reading into python with pandas. I can then create a mapping from rendered emojis to decriptions, and another
from unicode to descriptions, and replace emojis in tweets with these descriptions: 
<p><blockquote><pre>
#Load Emoji Unicode TSV (Make sure TBL is saved in txt file as utf-8)
emojis = pd.read_csv('C://Users/jliv/Downloads/emojis.txt',sep = '\t', encoding = 'utf-8')
#Map of Unicode and Names
emoji_map = pd.DataFrame()
emoji_map['name'] = emojis['Name(s)']
emoji_map['code'] = emojis['Escaped Unicode']
#Map of Emojis and names
emoji_map1 = pd.DataFrame()
emoji_map1['name'] = emojis['Name(s)']
emoji_map1['Emoji'] = emojis['Emoji']
#Handle escape characters in unicode
codes = [] 
for i in list(emojis['Escaped Unicode']):
    x = i.replace("\\","\\")
    codes.append(x)
    

emojislist = emoji_map1['Emoji']
#Convert CSVs of mappings to dict mappings
emoji_map.index = codes
emoji_dict = emoji_map.to_dict()
emoji_dict = emoji_dict['name']

emoji_map1.index = emojislist
emoji_dict1 = emoji_map1.to_dict()
emoji_dict1 = emoji_dict1['name']

#Replace tweet emojis and unicode with descriptions of characters
emoji_clean = []
for i in text:
    x = i
    for k,v in emoji_dict1.items():
        x = x.replace(k, v)
    for k,v in emoji_dict.items():
        x = x.replace(k, v)
    emoji_clean.append(x)
</pre></blockquote></p>
The next step in my cleaning exercise is to use regular expressions to recognize many website URLs that may
be present in tweets and remove these from the text. I search for 6 common URL patterns. I also remove 
largely non-sentimentized punctuation in the same phase of cleaning (e.g. commas, colons, parentheses):
<p><blockquote><pre>
tweetvector_clean = []
for i in emoji_clean:
    x = re.sub(r"^(http:\/\/www\.|https:\/\/www\.|http:\/\/|https:\/\/)?[a-z0-9]+([\-\.]{1}[a-z0-9]+)*\.[a-z]{2,5}(:[0-9]{1,5})?(\/.*)?$"," ", i)
    x = re.sub(r"htt\S+"," ", x) 
    x = re.sub(r"pic.twit\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"www.\S+"," ", x)
    x = re.sub(r"@\S+"," ", x)
    x = re.sub(r"\xa0"," ", x)
    x = re.sub(r"\\u\S+"," ", x)
    x = x.replace('#',' ')
    x = x.replace('amp;','&')
    x = x.replace('gt;',' ')
    x = x.replace('\\n',' ')
    y = x.replace('$',' ')
    y = y.replace('(',' ')
    y = y.replace('–',' ')
    y = y.replace('‘',' ')
    y = y.replace('“',' ')
    y = y.replace('”',' ')
    y = y.replace('`',' ')
    y = y.replace(']',' ')
    y = y.replace('[',' ')
    y = y.replace(';',' ')
    y = y.replace(')',' ')
    y = y.replace('/',' ')
    y = y.replace('*',' ')
    y = y.replace(',',' ')
    y = y.replace('’','')
    y = y.replace('.','')
    y = y.replace('-',' ')
    y = y.replace("'",'')
    y = y.replace(':',' ')
    y = y.replace('@',' ')
    y = y.replace('!',' ')
    y = y.replace('…',' ')
    y = y.replace('?',' ')
    y = y.replace('>',' ')
    y = y.replace('&',' ')
    y = y.replace("\\","")
    y = y.replace("\\u2066","")
    tweetvector_clean.append(y)
</pre></blockquote></p>
The next process in this text cleaning exercise is to tokenize documents so meaningless stopwords can be evaluated and
removed. I use the python Natural Language Tool Kit (NLTK) for much of this textual analysis, and I evaluate tweet 
stop words using a stop word bank from NLTK. I also pass tweets through an NLTK tokenization function for tokenizing:
<p><blockquote><pre>
Import nltk

stopwords = nltk.corpus.stopwords  
stop_words = set(stopwords.words("english"))
word_tokenize = nltk.tokenize.word_tokenize

#Tokenize to remove stopwords
tweetvector_tokenized = []
for i in tweetvector_clean:
    x = word_tokenize(i)
    tweetvector_tokenized.append(x)
tweetvector_stopped = []
for i in tweetvector_tokenized:
    newstatement = [j for j in i if j not in stop_words]
    tweetvector_stopped.append(newstatement)

#Rejoin tweets minus stopwords
final_tweets = []
for i in tweetvector_stopped:
    x = " ".join(i)
    final_tweets.append(x)
</pre></blockquote></p>
<h3>TF IDF and LDA for Top Classification</h3>
<br>

<h3>Code Snippets</h3>
Code Snippets Below<br><br>
<!--style>.indented {
  padding-left: 50pt;
  padding-right: 50pt;
}</style>
<style>.tab {
  padding-left: 50pt;
}</style>
<div class="indented"-->
<p><blockquote><pre>

</pre></blockquote></p>
<h3>Outcomes and Learnings</h3>
 
<h3>Summary and Takeaways</h3>
<br><br>

</p>
<li><a href="/projects/hill_fit.html">See the Script</a></li>
</body>
</html>